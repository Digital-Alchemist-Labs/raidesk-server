# RAiDesk Backend Server

AI-powered medical device regulatory assistant backend built with FastAPI, LangChain, and LangGraph.

## Features

### Core Features
- **Device Classification**: Analyzes device concepts to determine medical device status and risk classification
- **Purpose & Mechanism**: Generates detailed technical documentation for intended use and mechanism of action
- **4-Tier Planning**: Creates comprehensive regulatory strategies (Fastest, Normal, Conservative, Innovative)
- **Plan Refinement**: Refines plans based on user feedback and requirements
- **LangGraph Integration**: Stateful agent workflows for complex multi-step processes
- **Ollama Support**: Local LLM inference using Ollama

### New Enterprise Features ‚ú®
- **Session Management**: Track user sessions with persistent storage and automatic expiration
- **Plan Storage & Versioning**: Automatic plan storage with full version history
- **Advanced Error Handling**: Structured error responses with detailed debugging information
- **Rate Limiting**: Protect your API from abuse with configurable rate limits
- **Structured Logging**: JSON logging with request IDs and performance metrics
- **Flexible Storage**: Choose between SQLite (development) or Redis (production)
- **CORS Configuration**: Environment-based CORS for development and production
- **üî• Streaming Support (SSE)**: Real-time LLM response streaming for better UX

üìñ **See [SERVER_FEATURES.md](SERVER_FEATURES.md) for complete documentation**  
üìñ **See [MIGRATION_GUIDE.md](MIGRATION_GUIDE.md) for integration guide**  
üìñ **See [STREAMING_GUIDE.md](STREAMING_GUIDE.md) for streaming implementation** üÜï

## Architecture

```
raidesk-server/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ agents/              # LangGraph-based agents
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classifier.py       # Device classification
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ purpose.py          # Purpose & mechanism
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ planner.py          # Plan generation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ refiner.py          # Plan refinement
‚îÇ   ‚îú‚îÄ‚îÄ routers/             # FastAPI routers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classify.py         # Classification endpoint
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ purpose.py          # Purpose generation endpoint
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ standards.py        # Plan generation endpoint
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ refine.py           # Plan refinement endpoint
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sessions.py         # Session management ‚ú®
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ plans.py            # Plan management ‚ú®
‚îÇ   ‚îú‚îÄ‚îÄ storage/             # Storage layer ‚ú®
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py             # Storage interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sqlite_adapter.py   # SQLite implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ redis_adapter.py    # Redis implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ session_manager.py  # Session management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ plan_repository.py  # Plan storage & versioning
‚îÇ   ‚îú‚îÄ‚îÄ middleware/          # Middleware components ‚ú®
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ error_handler.py    # Global error handling
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging.py          # Structured logging
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rate_limiter.py     # Rate limiting
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ models.py            # Pydantic models
‚îÇ   ‚îú‚îÄ‚îÄ prompts.py           # Prompt templates
‚îÇ   ‚îú‚îÄ‚îÄ exceptions.py        # Custom exceptions ‚ú®
‚îÇ   ‚îú‚îÄ‚îÄ dependencies.py      # Dependency injection ‚ú®
‚îÇ   ‚îî‚îÄ‚îÄ main.py              # Main FastAPI app
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ env.example              # Environment template ‚ú®
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ SERVER_FEATURES.md       # Complete feature docs ‚ú®
‚îî‚îÄ‚îÄ MIGRATION_GUIDE.md       # Integration guide ‚ú®
```

## Prerequisites

- Python 3.10+
- Ollama with GPT-OSS model

### Install Ollama and GPT-OSS

```bash
# Install Ollama (if not already installed)
# Visit https://ollama.ai for installation instructions

# Pull GPT-OSS model
ollama pull gpt-oss
```

## Installation

1. **Clone the repository**

```bash
cd /Users/jaylee_83/Documents/_itsjayspace/git_clones/raidesk-server
```

2. **Create virtual environment**

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

4. **Configure environment**

```bash
cp env.example .env
# Edit .env file with your settings
```

## Configuration

Edit `.env` file:

```bash
# Server Configuration
HOST=0.0.0.0
PORT=8000
DEBUG=true

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=gpt-oss

# CORS
CORS_ORIGINS=http://localhost:3000,http://localhost:3001

# Storage (SQLite for development, Redis for production)
STORAGE_TYPE=sqlite
DATABASE_PATH=./raidesk.db
# REDIS_URL=redis://localhost:6379

# Session & Plan TTL
SESSION_TTL=86400    # 24 hours
PLAN_TTL=604800      # 7 days

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_PER_MINUTE=60

# LangSmith (Optional - for debugging)
# LANGCHAIN_TRACING_V2=true
# LANGCHAIN_API_KEY=your_api_key_here
# LANGCHAIN_PROJECT=raidesk
```

üìñ **See [env.example](env.example) for complete configuration options**

## Running the Server

### Development Mode

```bash
# Activate virtual environment
source venv/bin/activate

# Run with auto-reload
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

Or simply:

```bash
python app/main.py
```

### Production Mode

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
```

## API Endpoints

### Health Check

```
GET /         # API information
GET /health   # Health status with storage check
```

### Device Classification

```
POST /api/classify
```

**Request:**
```json
{
  "concept": "CT ÏòÅÏÉÅÏóêÏÑú ÌèêÍ≤∞Ï†àÏùÑ ÏûêÎèôÏúºÎ°ú Í≤ÄÏ∂úÌïòÎäî AI ÏÜåÌîÑÌä∏Ïõ®Ïñ¥",
  "context": "Ï∂îÍ∞Ä Ïª®ÌÖçÏä§Ìä∏ (ÏÑ†ÌÉùÏÇ¨Ìï≠)"
}
```

**Response:**
```json
{
  "classification": {
    "isMedicalDevice": true,
    "reasoning": "ÌåêÎã® Í∑ºÍ±∞",
    "confidence": 0.92,
    "category": "ÏòÅÏÉÅÏùòÌïô ÏßÑÎã®Î≥¥Ï°∞ ÏÜåÌîÑÌä∏Ïõ®Ïñ¥",
    "riskClass": "II"
  },
  "suggestedCategories": [...]
}
```

### Purpose & Mechanism

```
POST /api/purpose
```

**Request:**
```json
{
  "concept": "ÌèêÍ≤∞Ï†à Í≤ÄÏ∂ú AI",
  "category": "ÏòÅÏÉÅÏùòÌïô ÏßÑÎã®Î≥¥Ï°∞ ÏÜåÌîÑÌä∏Ïõ®Ïñ¥"
}
```

**Response:**
```json
{
  "intendedUse": "ÏÇ¨Ïö© Î™©Ï†Å",
  "mechanismOfAction": "ÏûëÏö© ÏõêÎ¶¨",
  "targetPopulation": "ÎåÄÏÉÅ ÌôòÏûêÍµ∞",
  "clinicalBenefit": "ÏûÑÏÉÅÏ†Å Ïù¥Ï†ê",
  "contraindications": ["Í∏àÍ∏∞ÏÇ¨Ìï≠"]
}
```

### Generate Plans

```
POST /api/standards
```

**Request:**
```json
{
  "classification": {...},
  "category": {...},
  "purposeMechanism": {...}
}
```

**Response:**
```json
{
  "plans": [
    {
      "id": "plan-fastest",
      "tier": "fastest",
      "title": "ÏµúÎã® Í≤ΩÎ°ú",
      "description": "...",
      "totalDuration": "6Í∞úÏõî",
      "estimatedCost": "1Ïñµ ~ 1.5ÏñµÏõê",
      "riskLevel": "high",
      "commonStandards": {...},
      "performanceEvaluation": {...},
      "pros": [...],
      "cons": [...],
      "recommendations": [...]
    },
    ...
  ]
}
```

### Refine Plan

```
POST /api/refine
```

**Request:**
```json
{
  "planId": "plan-fastest",
  "modifications": "ÎπÑÏö©ÏùÑ Îçî ÎÇÆÏ∂îÍ≥† Ïã∂ÏäµÎãàÎã§",
  "context": {
    "budget": 100000000
  }
}
```

> **Note:** Plans are now automatically retrieved from storage. No need to send `original_plan` in context!

### Session Management ‚ú®

```
POST   /api/sessions              # Create session
GET    /api/sessions              # List sessions
GET    /api/sessions/{id}         # Get session
PUT    /api/sessions/{id}         # Update session
DELETE /api/sessions/{id}         # Delete session
```

### Plan Management ‚ú®

```
GET    /api/plans                 # List all plans
GET    /api/plans/{id}            # Get plan
GET    /api/plans/{id}?version=1  # Get specific version
GET    /api/plans/{id}/record     # Get version history
DELETE /api/plans/{id}            # Delete plan
```

### Streaming Endpoints üî•

Real-time streaming for better user experience:

```
POST   /api/stream/classify       # Stream classification results
POST   /api/stream/purpose        # Stream purpose generation
POST   /api/stream/standards      # Stream plan generation
POST   /api/stream/refine         # Stream plan refinement
```

**Example (JavaScript):**
```javascript
const response = await fetch('http://localhost:8000/api/stream/classify', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ concept: 'Ïä§ÎßàÌä∏ Ïù∏ÏäêÎ¶∞ ÌéåÌîÑ' })
});

const reader = response.body.getReader();
const decoder = new TextDecoder();

while (true) {
  const { done, value } = await reader.read();
  if (done) break;
  
  const chunk = decoder.decode(value);
  console.log('Stream:', chunk);
}
```

üìñ **See [STREAMING_GUIDE.md](STREAMING_GUIDE.md) for complete examples and React hooks**

## API Documentation

Once the server is running, visit:

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## Technologies

- **[FastAPI](https://fastapi.tiangolo.com/)**: Modern, fast web framework
- **[LangChain](https://docs.langchain.com/oss/python/langchain/overview)**: LLM application framework
- **[LangGraph](https://docs.langchain.com/oss/python/langgraph/overview)**: State machine for agent workflows
- **[Ollama](https://ollama.ai)**: Local LLM inference
- **[Pydantic](https://docs.pydantic.dev/)**: Data validation

## LangGraph Agent Architecture

Each endpoint uses a LangGraph agent for stateful processing:

1. **Classifier Agent**: Multi-step classification with reasoning
2. **Purpose Agent**: Generates comprehensive technical documentation
3. **Planner Agent**: Creates 4 parallel regulatory strategies
4. **Refiner Agent**: Iteratively improves plans based on feedback

### Example Agent Flow

```python
from langgraph.graph import StateGraph, START, END

workflow = StateGraph(State)
workflow.add_node("classify", classify_node)
workflow.add_edge(START, "classify")
workflow.add_edge("classify", END)
agent = workflow.compile()

result = await agent.ainvoke({"concept": "..."})
```

## Development

### Project Structure

```python
app/
‚îú‚îÄ‚îÄ agents/          # LangGraph agents
‚îú‚îÄ‚îÄ routers/         # FastAPI endpoints
‚îú‚îÄ‚îÄ config.py        # Settings
‚îú‚îÄ‚îÄ models.py        # Pydantic schemas
‚îú‚îÄ‚îÄ prompts.py       # LLM prompts
‚îî‚îÄ‚îÄ main.py          # App entry point
```

### Adding a New Agent

1. Create agent file in `app/agents/`
2. Define state using `TypedDict`
3. Create nodes and edges
4. Compile workflow with `StateGraph`
5. Export agent function

### Adding a New Endpoint

1. Create router in `app/routers/`
2. Define request/response models in `app/models.py`
3. Import agent function
4. Add router to `app/main.py`

## Troubleshooting

### Ollama Connection Issues

```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Start Ollama (if not running)
ollama serve
```

### Model Not Found

```bash
# List available models
ollama list

# Pull GPT-OSS if missing
ollama pull gpt-oss
```

### CORS Issues

Update `CORS_ORIGINS` in `.env` to include your frontend URL:

```bash
CORS_ORIGINS=http://localhost:3000,http://localhost:3001
```

## License

See LICENSE file for details.

## References

- [LangChain Documentation](https://docs.langchain.com/oss/python/langchain/overview)
- [LangGraph Documentation](https://docs.langchain.com/oss/python/langgraph/overview)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Ollama Documentation](https://github.com/ollama/ollama)

## Support

For issues and questions, please open an issue on GitHub.
